{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf65ee39-8d62-42ed-aae0-434ef20bc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99e57cee-41c8-4c36-a61b-225e6a61d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(resume_path, job_path, resume_sample=1000, job_sample=5000):\n",
    "    # Load resumes and get random sample if too long\n",
    "    resume_df = pd.read_csv(resume_path)\n",
    "    if resume_sample and resume_sample < len(resume_df):\n",
    "        resume_df = resume_df.sample(resume_sample, random_state=42)\n",
    "\n",
    "    # Load jobs and get random samples if needed\n",
    "    job_df = pd.read_csv(job_path)\n",
    "    if job_sample and job_sample < len(job_df):\n",
    "        job_df = job_df.sample(job_sample, random_state=42)\n",
    "\n",
    "    return resume_df, job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02446eba-7246-4432-a1df-0ef3cf80a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    # Remove punctuation\n",
    "    keep_chars = 'abcdefghijklmnopqrestuvwxyz0123456789_+# .'\n",
    "    cleaned_text = ''\n",
    "    for char in text:\n",
    "        if char in keep_chars:\n",
    "            cleaned_text += char\n",
    "        else:\n",
    "            cleaned_text += ' '\n",
    "    # Remove whitespace\n",
    "    words = cleaned_text.split()\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8c7c14b-626b-4356-b12b-b1dd76a84738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(resume_df, job_df, resume_col='Resume_str', job_col='Job Description'):\n",
    "    # Preprocess\n",
    "    resume_text =resume_df[resume_col].fillna('').apply(preprocess)\n",
    "    job_text = job_df[job_col].fillna('').apply(preprocess)\n",
    "\n",
    "    if 'Job Title'in job_df.columns:\n",
    "        job_title = job_df['Job Title'].fillna('').apply(preprocess)\n",
    "        job_text = job_title + ' ' + job_text\n",
    "\n",
    "    all_text = list(resume_text) + list(job_text)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.7,ngram_range=(1,2), stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "\n",
    "    resume_vectors = tfidf_matrix[:len(resume_df)]\n",
    "    job_vectors = tfidf_matrix[len(resume_df):]\n",
    "\n",
    "    return resume_vectors, job_vectors, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99eaefcd-31c8-41d4-985f-27813d3e3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity for batch\n",
    "def process_similarity_batch(resume_batch, job_vectors, batch_start_idx, threshold):\n",
    "    similarity_matrix = cosine_similarity(resume_batch, job_vectors)\n",
    "\n",
    "    batch_similarities = []\n",
    "\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        resume_idx = batch_start_idx + i\n",
    "        for job_idx, score in enumerate(similarity_matrix[i]):\n",
    "            if score > threshold:\n",
    "                batch_similarities.append((resume_idx, job_idx, float(score)))\n",
    "    return batch_similarities\n",
    "\n",
    "def similarity_calculation(resume_vectors, job_vectors, batch_size=100, threshold=0.1):\n",
    "    n_resumes =resume_vectors.shape[0]\n",
    "    n_jobs = job_vectors.shape[0]\n",
    "    similarity_pairs = []\n",
    "\n",
    "    total_batches = (n_resumes + batch_size - 1)//batch_size\n",
    "\n",
    "    for batch_start in range(0,n_resumes, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, n_resumes)\n",
    "        resume_batch = resume_vectors[batch_start:batch_end]\n",
    "\n",
    "        batch_similarities = process_similarity_batch(resume_batch, job_vectors, batch_start,threshold)\n",
    "\n",
    "        similarity_pairs.extend(batch_similarities)\n",
    "    return similarity_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8fb30a0-e63c-4365-af90-e2736e8c0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positive and negative training pairs\n",
    "def creat_training_pairs(similarities, resume_df, job_df, top_k=10, neg_ratio=2):\n",
    "    resume_to_job = {}\n",
    "    for resume_idx, job_idx, sim in similarities:\n",
    "        if resume_idx not in resume_to_job:\n",
    "            resume_to_job[resume_idx] = []\n",
    "        resume_to_job[resume_idx].append((job_idx, sim))\n",
    "\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "\n",
    "    for resume_idx, matches in resume_to_job.items():\n",
    "        resume_id = resume_df.iloc[resume_idx].get('ID', resume_idx)\n",
    "        matches.sort(key=lambda x:x[:1], reverse=True)\n",
    "\n",
    "        # positive matches\n",
    "        pos_count = min(top_k, len(matches))\n",
    "        pos_job_ids = []\n",
    "\n",
    "        for job_idx, sim in matches[:pos_count]:\n",
    "            job_id = job_df.iloc[job_idx].get('Job Id', job_idx)\n",
    "            positive_pairs.append((resume_id, job_id, sim))\n",
    "            pos_job_ids.append(job_id)\n",
    "\n",
    "        # negative pairs (middle=hard, bottom=soft)\n",
    "        neg_count = min(pos_count * neg_ratio, len(matches) - pos_count)\n",
    "\n",
    "        if neg_count > 0:\n",
    "            mid_start = pos_count\n",
    "            mid_end = min(pos_count + neg_count // 2, len(matches) - neg_count //2)\n",
    "\n",
    "            for job_idx, sim in matches[mid_start:mid_end]:\n",
    "                job_id = job_df.iloc[job_idx].get('Job Id', job_idx)\n",
    "                if job_id not in pos_job_ids:\n",
    "                    negative_pairs.append((resume_id, job_id, sim))\n",
    "\n",
    "            remaining = neg_count - (mid_end - mid_start)\n",
    "            if remaining > 0:\n",
    "                for job_idx, sim in matches[-remaining:]:\n",
    "                    job_id = job_df.iloc[job_idx].get('Job Id', job_idx)\n",
    "                    if job_id not in pos_job_ids:\n",
    "                        negative_pairs.append((resume_id, job_id, sim))\n",
    "\n",
    "    return positive_pairs, negative_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fe090f1-022e-4263-ad2d-17b9f99908f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desktop path\n",
    "#resume_path = '/home/gv/school/trustworthy_ai/proj/resume_data/archive/Resume/Resume.csv'\n",
    "#job_path = '/home/gv/school/trustworthy_ai/proj/job_data/job_descriptions.csv'\n",
    "#output_path = '/home/gv/school/trustworthy_ai/proj/resume_job_matching_trustworthy_ai/resume_job_data/resume_job_pairs.csv'\n",
    "\n",
    "# mac path\n",
    "resume_path = '/Users/gv/code/school/trustworthy_ai/archive/Resume/Resume.csv'\n",
    "job_path = '/Users/gv/code/school/trustworthy_ai/archive/Jobs/job_descriptions.csv'\n",
    "output_path = '/Users/gv/code/school/trustworthy_ai/archive/training_pairs/resume_job_pairs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abc5b3f3-4426-4ac5-b390-bebfe54cbf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df, job_df = load_data(resume_path, job_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ac79efd-e361-4c99-8c23-aa8833f20fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_vectors, job_vectors, _ =vectorize(resume_df, job_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cad2a17d-9d4a-4f2b-ad17-67696867161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = similarity_calculation(resume_vectors, job_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04f18560-dbef-48b6-a6ae-95743e2ed75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pairs, neg_pairs = creat_training_pairs(similarities, resume_df, job_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c800ef3a-4fe9-4582-b00e-7bbbddcaf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(pos_pairs, columns=['resume_id', 'job_id', 'score'])\n",
    "pos_df['label'] = 1\n",
    "neg_df = pd.DataFrame(neg_pairs, columns=['resume_id', 'job_id', 'score'])\n",
    "neg_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a315f0ac-349c-41a5-b540-06c3e5a23245",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "training_pairs_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52847d86-4eae-4e4f-959f-f2af8c1a31f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
