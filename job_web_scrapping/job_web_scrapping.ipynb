{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program purpose is to scrap job listing for specific locations and specific job titles or categories.\n",
    "We will scrap job listing from linkedin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting fpdf2\n",
      "  Downloading fpdf2-2.8.2-py2.py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting defusedxml (from fpdf2)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting Pillow!=9.2.*,>=6.2.2 (from fpdf2)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting fonttools>=4.34.0 (from fpdf2)\n",
      "  Using cached fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Downloading fpdf2-2.8.2-py2.py3-none-any.whl (236 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: Pillow, fonttools, defusedxml, fpdf2\n",
      "Successfully installed Pillow-11.1.0 defusedxml-0.7.1 fonttools-4.56.0 fpdf2-2.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Installing the packages we need\n",
    "\"\"\"\n",
    "%pip install beautifulsoup4\n",
    "%pip install requests\n",
    "%pip install pandas\n",
    "%pip install fpdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franz2897/Documents/usf_master/trustworthyAI/resume_job_matching_trustworthy_ai/venv/lib/python3.12/site-packages/fpdf/__init__.py:40: UserWarning: You have both PyFPDF & fpdf2 installed. Both packages cannot be installed at the same time as they share the same module namespace. To only keep fpdf2, run: pip uninstall --yes pypdf && pip install --upgrade fpdf2\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 has 133 jobs\n",
      "Fetching jobs, start: 25 for url: https://www.linkedin.com/jobs/search?keywords=['engineer', 'healthcare', 'finance']&location=US&start=25\n",
      "job_id_list length: 7\n",
      "Page 1 has 133 jobs\n",
      "Fetching jobs, start: 50 for url: https://www.linkedin.com/jobs/search?keywords=['engineer', 'healthcare', 'finance']&location=US&start=50\n",
      "job_id_list length: 14\n",
      "Page 1 has 0 jobs\n",
      "Fetching jobs, start: 75 for url: https://www.linkedin.com/jobs/search?keywords=['engineer', 'healthcare', 'finance']&location=US&start=75\n",
      "job_id_list length: 14\n",
      "job id list: [{'job_id': '4150517213', 'keyword': 'engineer'}, {'job_id': '4172354351', 'keyword': 'engineer'}, {'job_id': '3795745978', 'keyword': 'engineer'}, {'job_id': '4174580176', 'keyword': 'engineer'}, {'job_id': '4173021747', 'keyword': 'engineer'}, {'job_id': '4170308528', 'keyword': 'engineer'}, {'job_id': '4088126303', 'keyword': 'engineer'}, {'job_id': '4150517213', 'keyword': 'healthcare'}, {'job_id': '4172354351', 'keyword': 'healthcare'}, {'job_id': '3795745978', 'keyword': 'healthcare'}, {'job_id': '4174580176', 'keyword': 'healthcare'}, {'job_id': '4173021747', 'keyword': 'healthcare'}, {'job_id': '4170308528', 'keyword': 'healthcare'}, {'job_id': '4088126303', 'keyword': 'healthcare'}]\n",
      "Fetching job details for job_id: 4150517213\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4150517213\n",
      "Fetching job details for job_id: 4172354351\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4172354351\n",
      "Fetching job details for job_id: 3795745978\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3795745978\n",
      "Fetching job details for job_id: 4174580176\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4174580176\n",
      "Fetching job details for job_id: 4173021747\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4173021747\n",
      "Fetching job details for job_id: 4170308528\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4170308528\n",
      "Fetching job details for job_id: 4088126303\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4088126303\n",
      "Fetching job details for job_id: 4150517213\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4150517213\n",
      "Fetching job details for job_id: 4172354351\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4172354351\n",
      "Fetching job details for job_id: 3795745978\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3795745978\n",
      "Fetching job details for job_id: 4174580176\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4174580176\n",
      "Fetching job details for job_id: 4173021747\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4173021747\n",
      "Fetching job details for job_id: 4170308528\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4170308528\n",
      "Fetching job details for job_id: 4088126303\n",
      "Fetch url: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4088126303\n",
      "Total jobs fetched: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13202/233971510.py:136: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", size=12)\n",
      "/tmp/ipykernel_13202/233971510.py:144: DeprecationWarning: \"dest\" parameter is deprecated since v2.2.0, unused and will soon be removed\n",
      "  pdf.output(pdf_file_path, 'F')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from fpdf import FPDF\n",
    "\"\"\"\n",
    "Modify parameters as needed\n",
    "\"\"\"\n",
    "number_of_pages = 1\n",
    "keywords =  ['engineer','healthcare','finance']\n",
    "location = 'US'\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sample_job_listing_url = f\"https://www.linkedin.com/jobs/search?keywords={keywords}&location={location}\"\n",
    "job_post_list = []\n",
    "counter = 0\n",
    "job_id_list = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    for i in range(0, number_of_pages):\n",
    "        \"\"\"\n",
    "        Getting list of job posting by searching for <li> element\n",
    "        \"\"\"\n",
    "        counter += 25\n",
    "        current_page_listing_url = f\"{sample_job_listing_url}&start={counter}\"\n",
    "        response = requests.get(sample_job_listing_url)\n",
    "        list_data = response.text\n",
    "        # Parsing the HTML response\n",
    "        list_soup = BeautifulSoup(list_data, 'html.parser')\n",
    "        page_jobs = list_soup.find_all('li')\n",
    "        print(f\"Page {i+1} has {len(page_jobs)} jobs\")\n",
    "        print(\n",
    "            f'Fetching jobs, start: {counter} for url: {current_page_listing_url}')\n",
    "        # Extracting information from each job\n",
    "        for job in page_jobs:\n",
    "            \"\"\"\n",
    "            For each <li> we get the job id\n",
    "            Example job post data-entity-urn=\"urn:li:jobPosting:4179181538\"\n",
    "            \"\"\"\n",
    "            base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
    "            if base_card_div is None:\n",
    "                continue\n",
    "            # This splitting might not work if they change the format of the job_id\n",
    "            job_id = base_card_div.get('data-entity-urn').split(\":\")[3]\n",
    "            job_id_list.append({\"job_id\": job_id, \"keyword\": keyword})\n",
    "            \"\"\"\n",
    "            Now that we have the job ids , we are going to fetch the job details\n",
    "            Sample URL: https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/4179518973\n",
    "            \"\"\"\n",
    "        print(f\"job_id_list length: {len(job_id_list)}\")\n",
    "        \"\"\"\n",
    "        Once we get all the job ids, we start fetching the job details\n",
    "        \"\"\"\n",
    "print(f\"job id list: {job_id_list}\")\n",
    "for jobs in job_id_list:\n",
    "        job_id = jobs['job_id']\n",
    "        print(f\"Fetching job details for job_id: {job_id}\")\n",
    "        job_details_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "        print(f\"Fetch url: {job_details_url}\")\n",
    "        response = requests.get(job_details_url)\n",
    "        job_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        job_post = {}\n",
    "        # Getting company name\n",
    "        try:\n",
    "            company_name = job_soup.find(\n",
    "                \"a\", {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"}).text.strip()\n",
    "        except:\n",
    "            company_name = None\n",
    "            continue\n",
    "        job_post['company_name'] = company_name\n",
    "        # Getting position name\n",
    "        try:\n",
    "            position_name = job_soup.find(\n",
    "                \"h2\", {\"class\": \"topcard__title\"}).text.strip()\n",
    "        except:\n",
    "            position_name = ''\n",
    "            continue\n",
    "        job_post['position_name'] = position_name\n",
    "        # Getting published date\n",
    "        try:\n",
    "            posted_time_ago = job_soup.find(\n",
    "                \"span\", {\"class\": \"posted-time-ago__text topcard__flavor--metadata\"}).text.strip()\n",
    "        except:\n",
    "            posted_time_ago = ''\n",
    "        job_post['posted_time_ago'] = posted_time_ago\n",
    "        job_post['web scrapped date'] = datetime.now().strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S\")\n",
    "        # Getting number of applicants\n",
    "        try:\n",
    "            num_applicants = job_soup.find(\n",
    "                \"figcaption\", {\"class\": \"num-applicants__caption\"}).text.strip()\n",
    "        except:\n",
    "            num_applicants = ''\n",
    "        job_post['num_applicants'] = num_applicants\n",
    "\n",
    "        # Getting job full description html\n",
    "        try:\n",
    "            job_description_html = job_soup.find(\n",
    "                \"div\", {\"class\": \"decorated-job-posting__details\"})\n",
    "            job_description_text = job_description_html.find(\n",
    "                \"div\", {\"class\": \"show-more-less-html__markup\"}).get_text().strip()\n",
    "        except:\n",
    "            print(f\"Error skipping because not job description found: {job_id}\")\n",
    "            continue\n",
    "        job_post['job_description_html'] = job_description_html\n",
    "        job_post['job_description_text'] = job_description_text\n",
    "        job_post['job_id'] = job_id\n",
    "        job_post['keyword'] = jobs['keyword']\n",
    "        job_post_list.append(job_post)\n",
    "print(f\"Total jobs fetched: {len(job_post_list)}\")\n",
    "job_list_df = pd.DataFrame(job_post_list)\n",
    "if len(job_list_df) > 0:\n",
    "    # Create a folder for each keyword\n",
    "    prefix = '../samples/job_listings/'\n",
    "    for keyword in job_list_df['keyword'].unique():\n",
    "        folder_path = f\"{prefix}{keyword}_jobs\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Save job descriptions as PDF files\n",
    "    for index, row in job_list_df.iterrows():\n",
    "        keyword = row['keyword']\n",
    "        job_id = row['job_id']\n",
    "        job_description_text = row['job_description_text']\n",
    "        \n",
    "        # Create a PDF object\n",
    "        pdf = FPDF()\n",
    "        \n",
    "        # Add a page\n",
    "        pdf.add_page()\n",
    "        \n",
    "        # Set font and size\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "        job_description_text = re.sub(r'[^\\x00-\\x7F]+', \"'\", job_description_text) \n",
    "        # Add job description text to the PDF\n",
    "        pdf.multi_cell(0, 10, job_description_text)\n",
    "        \n",
    "        # Save the PDF file in the corresponding folder\n",
    "        short_desc = job_description_text[:20].replace(\" \", \"_\")\n",
    "        pdf_file_path = f\"{prefix}{keyword}_jobs/{job_id}_{short_desc}.pdf\"\n",
    "        pdf.output(pdf_file_path, 'F')\n",
    "    \n",
    "    \n",
    "    job_list_df.to_csv(\n",
    "    f\"location: {location}, date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
